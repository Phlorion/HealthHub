{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50356ed7",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "629dd349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vniko\\anaconda3\\envs\\llm_hugging_face\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model...\n",
      "LoRA adapters loaded from ./flan_t5_fine_tuned_model.\n",
      "Running model on CPU.\n",
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel # Crucial for loading LoRA adapters\n",
    "\n",
    "# --- Global Model Loading (occurs once when the app starts) ---\n",
    "tokenizer = None\n",
    "model = None\n",
    "# This path must match the 'output_dir' from your training script\n",
    "MODEL_SAVE_PATH = \"./flan_t5_fine_tuned_model\"\n",
    "# This is the name of the original base model you used for fine-tuning\n",
    "BASE_MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "try:\n",
    "    print(\"Loading tokenizer and model...\")\n",
    "    # 1. Load the tokenizer that was saved during training (it's the same as the base model's tokenizer)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH) # Load saved tokenizer for consistency\n",
    "\n",
    "    # 2. Load the base FLAN-T5 model\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "    # 3. Load the LoRA adapters and apply them to the base model\n",
    "    # The 'MODEL_SAVE_PATH' directory contains the LoRA adapter weights (e.g., 'adapter_model.bin' or 'adapter_model.safetensors')\n",
    "    if os.path.exists(MODEL_SAVE_PATH) and os.path.isdir(MODEL_SAVE_PATH):\n",
    "        model = PeftModel.from_pretrained(base_model, MODEL_SAVE_PATH)\n",
    "        print(f\"LoRA adapters loaded from {MODEL_SAVE_PATH}.\")\n",
    "        # Optional: If you want to merge the LoRA adapters into the base model's weights\n",
    "        # for potentially slightly faster inference and a self-contained model,\n",
    "        # you can uncomment the line below. This makes the model larger in memory.\n",
    "        # model = model.merge_and_unload()\n",
    "        # print(\"LoRA adapters merged into the base model.\")\n",
    "    else:\n",
    "        print(f\"Warning: LoRA adapters not found at '{MODEL_SAVE_PATH}'. \"\n",
    "              \"Ensure training script was run and directory exists. \"\n",
    "              \"Falling back to original base model (will not have fine-tuned behavior).\", file=sys.stderr)\n",
    "        model = base_model # Fallback if adapters aren't found. In production, you might exit.\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode (crucial for inference)\n",
    "\n",
    "    # Move the model to GPU if available for faster predictions\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(\"cuda\")\n",
    "        print(\"Model moved to GPU.\")\n",
    "    else:\n",
    "        print(\"Running model on CPU.\")\n",
    "\n",
    "    print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\", file=sys.stderr)\n",
    "    sys.exit(1) # Exit if model loading fails, as the app cannot function without it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8adb84",
   "metadata": {},
   "source": [
    "# Run Flask Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b0bca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://192.168.1.11:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "# --- Flask Routes ---\n",
    "app = Flask(__name__)\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Get data from the POST request\n",
    "        data = request.get_json()\n",
    "\n",
    "        # Input validation: Ensure 'text' key exists and is a string\n",
    "        if not data or 'text' not in data or not isinstance(data['text'], str):\n",
    "            return jsonify({'error': \"Invalid request. Please provide a JSON body with a 'text' field (string).\"}), 400\n",
    "\n",
    "        input_text = data['text']\n",
    "\n",
    "        # --- Preprocess input text for your model ---\n",
    "        # IMPORTANT: Use the exact same instruction prefix as during training!\n",
    "        input_for_model = f\"Identify intent and filters: {input_text}\"\n",
    "        inputs = tokenizer(\n",
    "            input_for_model,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\", # Ensure consistent input length\n",
    "            max_length=128        # Match max_length used during training preprocessing\n",
    "        )\n",
    "\n",
    "        # Move input tensors to the same device as the model (GPU/CPU)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "        # --- Make prediction ---\n",
    "        with torch.no_grad(): # Disable gradient calculation for inference (saves memory and speeds up)\n",
    "            # Use the .generate() method for Seq2Seq models like T5\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=128, # Max length of the generated output (adjust as needed)\n",
    "                num_beams=5,        # Use beam search for potentially better quality (can be adjusted)\n",
    "                early_stopping=True # Stop generation if all beam hypotheses have finished\n",
    "            )\n",
    "\n",
    "        # Decode the generated output back to a human-readable string\n",
    "        prediction_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # --- Return the prediction as a JSON response ---\n",
    "        print(prediction_output)\n",
    "        return jsonify({'prediction': prediction_output})\n",
    "\n",
    "    except KeyError as e:\n",
    "        # Specific error for missing 'text' key in the JSON\n",
    "        return jsonify({'error': f\"Missing expected data field: {e}. Please provide a JSON body with a 'text' key.\"}), 400\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected errors during processing or prediction\n",
    "        print(f\"An error occurred during prediction: {e}\", file=sys.stderr) # Log the full error on the server side\n",
    "        return jsonify({'error': \"An internal server error occurred during prediction. Please try again later.\"}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run the Flask app on a specified host and port.\n",
    "    # '0.0.0.0' makes it accessible from other devices on your local network.\n",
    "    # Set debug=False for production environments.\n",
    "    app.run(host='192.168.1.11', port=5000, debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_hugging_face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
